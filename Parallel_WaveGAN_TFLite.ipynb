{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoSnqSTPHhHz"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tulasiram58827/TTS_TFLite/blob/main/Parallel_WaveGAN_TFLite.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uiVPAgj4-dK"
   },
   "source": [
    "This notebook converts PyTorch pre-trained model of Parallel WaveGAN to TFLite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0y3k1MM5WyE"
   },
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bh8yp58n5YpI"
   },
   "source": [
    "Parallel WaveGAN(Vocoder) is a waveform generation method from spectrogram using a Generative Adversial Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXsdqG7J6wN7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDNRXBlj6kZy"
   },
   "outputs": [],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxruntime\n",
    "!pip install pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbJwk8QH7a_P"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRbdzyHF7c8D"
   },
   "source": [
    "*Below utilities are taken from [Parallel WaveGAN Repository](https://github.com/kan-bayashi/ParallelWaveGAN)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXZ4nyZS7Zwz"
   },
   "outputs": [],
   "source": [
    "class Conv1d(torch.nn.Conv1d):\n",
    "    \"\"\"Conv1d module with customized initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize Conv1d module.\"\"\"\n",
    "        super(Conv1d, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset parameters.\"\"\"\n",
    "        torch.nn.init.kaiming_normal_(self.weight, nonlinearity=\"relu\")\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.constant_(self.bias, 0.0)\n",
    "\n",
    "\n",
    "class Conv1d1x1(Conv1d):\n",
    "    \"\"\"1x1 Conv1d with customized initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bias):\n",
    "        \"\"\"Initialize 1x1 Conv1d module.\"\"\"\n",
    "        super(Conv1d1x1, self).__init__(in_channels, out_channels,\n",
    "                                        kernel_size=1, padding=0,\n",
    "                                        dilation=1, bias=bias)\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \"\"\"Residual block module in WaveNet.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel_size=3,\n",
    "                 residual_channels=64,\n",
    "                 gate_channels=128,\n",
    "                 skip_channels=64,\n",
    "                 aux_channels=80,\n",
    "                 dropout=0.0,\n",
    "                 dilation=1,\n",
    "                 bias=True,\n",
    "                 use_causal_conv=False\n",
    "                 ):\n",
    "        \"\"\"Initialize ResidualBlock module.\n",
    "        Args:\n",
    "            kernel_size (int): Kernel size of dilation convolution layer.\n",
    "            residual_channels (int): Number of channels for residual connection.\n",
    "            skip_channels (int): Number of channels for skip connection.\n",
    "            aux_channels (int): Local conditioning channels i.e. auxiliary input dimension.\n",
    "            dropout (float): Dropout probability.\n",
    "            dilation (int): Dilation factor.\n",
    "            bias (bool): Whether to add bias parameter in convolution layers.\n",
    "            use_causal_conv (bool): Whether to use use_causal_conv or non-use_causal_conv convolution.\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # no future time stamps available\n",
    "        if use_causal_conv:\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "        else:\n",
    "            assert (kernel_size - 1) % 2 == 0, \"Not support even number kernel size.\"\n",
    "            padding = (kernel_size - 1) // 2 * dilation\n",
    "        self.use_causal_conv = use_causal_conv\n",
    "\n",
    "        # dilation conv\n",
    "        self.conv = Conv1d(residual_channels, gate_channels, kernel_size,\n",
    "                           padding=padding, dilation=dilation, bias=bias)\n",
    "\n",
    "        # local conditioning\n",
    "        if aux_channels > 0:\n",
    "            self.conv1x1_aux = Conv1d1x1(aux_channels, gate_channels, bias=False)\n",
    "        else:\n",
    "            self.conv1x1_aux = None\n",
    "\n",
    "        # conv output is split into two groups\n",
    "        gate_out_channels = gate_channels // 2\n",
    "        self.conv1x1_out = Conv1d1x1(gate_out_channels, residual_channels, bias=bias)\n",
    "        self.conv1x1_skip = Conv1d1x1(gate_out_channels, skip_channels, bias=bias)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor (B, residual_channels, T).\n",
    "            c (Tensor): Local conditioning auxiliary tensor (B, aux_channels, T).\n",
    "        Returns:\n",
    "            Tensor: Output tensor for residual connection (B, residual_channels, T).\n",
    "            Tensor: Output tensor for skip connection (B, skip_channels, T).\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # remove future time steps if use_causal_conv conv\n",
    "        x = x[:, :, :residual.size(-1)] if self.use_causal_conv else x\n",
    "\n",
    "        # split into two part for gated activation\n",
    "        splitdim = 1\n",
    "        xa, xb = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "\n",
    "        # local conditioning\n",
    "        if c is not None:\n",
    "            assert self.conv1x1_aux is not None\n",
    "            c = self.conv1x1_aux(c)\n",
    "            ca, cb = c.split(c.size(splitdim) // 2, dim=splitdim)\n",
    "            xa, xb = xa + ca, xb + cb\n",
    "\n",
    "        x = torch.tanh(xa) * torch.sigmoid(xb)\n",
    "\n",
    "        # for skip connection\n",
    "        s = self.conv1x1_skip(x)\n",
    "\n",
    "        # for residual connection\n",
    "        x = (self.conv1x1_out(x) + residual) * math.sqrt(0.5)\n",
    "\n",
    "        return x, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSBb0MFA7xmx"
   },
   "outputs": [],
   "source": [
    "class Stretch2d(torch.nn.Module):\n",
    "    \"\"\"Stretch2d module.\"\"\"\n",
    "\n",
    "    def __init__(self, x_scale, y_scale, mode=\"nearest\"):\n",
    "        \"\"\"Initialize Stretch2d module.\n",
    "        Args:\n",
    "            x_scale (int): X scaling factor (Time axis in spectrogram).\n",
    "            y_scale (int): Y scaling factor (Frequency axis in spectrogram).\n",
    "            mode (str): Interpolation mode.\n",
    "        \"\"\"\n",
    "        super(Stretch2d, self).__init__()\n",
    "        self.x_scale = x_scale\n",
    "        self.y_scale = y_scale\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor (B, C, F, T).\n",
    "        Returns:\n",
    "            Tensor: Interpolated tensor (B, C, F * y_scale, T * x_scale),\n",
    "        \"\"\"\n",
    "        return F.interpolate(\n",
    "            x, scale_factor=(self.y_scale, self.x_scale), mode=self.mode)\n",
    "\n",
    "\n",
    "class Conv2d(torch.nn.Conv2d):\n",
    "    \"\"\"Conv2d module with customized initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize Conv2d module.\"\"\"\n",
    "        super(Conv2d, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset parameters.\"\"\"\n",
    "        self.weight.data.fill_(1. / np.prod(self.kernel_size))\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.constant_(self.bias, 0.0)\n",
    "\n",
    "\n",
    "class UpsampleNetwork(torch.nn.Module):\n",
    "    \"\"\"Upsampling network module.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 upsample_scales,\n",
    "                 nonlinear_activation=None,\n",
    "                 nonlinear_activation_params={},\n",
    "                 interpolate_mode=\"nearest\",\n",
    "                 freq_axis_kernel_size=1,\n",
    "                 use_causal_conv=False,\n",
    "                 ):\n",
    "        \"\"\"Initialize upsampling network module.\n",
    "        Args:\n",
    "            upsample_scales (list): List of upsampling scales.\n",
    "            nonlinear_activation (str): Activation function name.\n",
    "            nonlinear_activation_params (dict): Arguments for specified activation function.\n",
    "            interpolate_mode (str): Interpolation mode.\n",
    "            freq_axis_kernel_size (int): Kernel size in the direction of frequency axis.\n",
    "        \"\"\"\n",
    "        super(UpsampleNetwork, self).__init__()\n",
    "        self.use_causal_conv = use_causal_conv\n",
    "        self.up_layers = torch.nn.ModuleList()\n",
    "        for scale in upsample_scales:\n",
    "            # interpolation layer\n",
    "            stretch = Stretch2d(scale, 1, interpolate_mode)\n",
    "            self.up_layers += [stretch]\n",
    "\n",
    "            # conv layer\n",
    "            assert (freq_axis_kernel_size - 1) % 2 == 0, \"Not support even number freq axis kernel size.\"\n",
    "            freq_axis_padding = (freq_axis_kernel_size - 1) // 2\n",
    "            kernel_size = (freq_axis_kernel_size, scale * 2 + 1)\n",
    "            if use_causal_conv:\n",
    "                padding = (freq_axis_padding, scale * 2)\n",
    "            else:\n",
    "                padding = (freq_axis_padding, scale)\n",
    "            conv = Conv2d(1, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "            self.up_layers += [conv]\n",
    "\n",
    "            # nonlinear\n",
    "            if nonlinear_activation is not None:\n",
    "                nonlinear = getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)\n",
    "                self.up_layers += [nonlinear]\n",
    "\n",
    "    def forward(self, c):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "        Args:\n",
    "            c : Input tensor (B, C, T).\n",
    "        Returns:\n",
    "            Tensor: Upsampled tensor (B, C, T'), where T' = T * prod(upsample_scales).\n",
    "        \"\"\"\n",
    "        c = c.unsqueeze(1)  # (B, 1, C, T)\n",
    "        for f in self.up_layers:\n",
    "            if self.use_causal_conv and isinstance(f, Conv2d):\n",
    "                c = f(c)[..., :c.size(-1)]\n",
    "            else:\n",
    "                c = f(c)\n",
    "        return c.squeeze(1)  # (B, C, T')\n",
    "class ConvInUpsampleNetwork(torch.nn.Module):\n",
    "    \"\"\"Convolution + upsampling network module.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 upsample_scales,\n",
    "                 nonlinear_activation=None,\n",
    "                 nonlinear_activation_params={},\n",
    "                 interpolate_mode=\"nearest\",\n",
    "                 freq_axis_kernel_size=1,\n",
    "                 aux_channels=80,\n",
    "                 aux_context_window=0,\n",
    "                 use_causal_conv=False\n",
    "                 ):\n",
    "        \"\"\"Initialize convolution + upsampling network module.\n",
    "        Args:\n",
    "            upsample_scales (list): List of upsampling scales.\n",
    "            nonlinear_activation (str): Activation function name.\n",
    "            nonlinear_activation_params (dict): Arguments for specified activation function.\n",
    "            mode (str): Interpolation mode.\n",
    "            freq_axis_kernel_size (int): Kernel size in the direction of frequency axis.\n",
    "            aux_channels (int): Number of channels of pre-convolutional layer.\n",
    "            aux_context_window (int): Context window size of the pre-convolutional layer.\n",
    "            use_causal_conv (bool): Whether to use causal structure.\n",
    "        \"\"\"\n",
    "        super(ConvInUpsampleNetwork, self).__init__()\n",
    "        self.aux_context_window = aux_context_window\n",
    "        self.use_causal_conv = use_causal_conv and aux_context_window > 0\n",
    "        # To capture wide-context information in conditional features\n",
    "        kernel_size = aux_context_window + 1 if use_causal_conv else 2 * aux_context_window + 1\n",
    "        # NOTE(kan-bayashi): Here do not use padding because the input is already padded\n",
    "        self.conv_in = Conv1d(aux_channels, aux_channels, kernel_size=kernel_size, bias=False)\n",
    "        self.upsample = UpsampleNetwork(\n",
    "            upsample_scales=upsample_scales,\n",
    "            nonlinear_activation=nonlinear_activation,\n",
    "            nonlinear_activation_params=nonlinear_activation_params,\n",
    "            interpolate_mode=interpolate_mode,\n",
    "            freq_axis_kernel_size=freq_axis_kernel_size,\n",
    "            use_causal_conv=use_causal_conv,\n",
    "        )\n",
    "\n",
    "    def forward(self, c):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "        Args:\n",
    "            c : Input tensor (B, C, T').\n",
    "        Returns:\n",
    "            Tensor: Upsampled tensor (B, C, T),\n",
    "                where T = (T' - aux_context_window * 2) * prod(upsample_scales).\n",
    "        Note:\n",
    "            The length of inputs considers the context window size.\n",
    "        \"\"\"\n",
    "        c_ = self.conv_in(c)\n",
    "        c = c_[:, :, :-self.aux_context_window] if self.use_causal_conv else c_\n",
    "        return self.upsample(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pREo8Mo74Jm"
   },
   "outputs": [],
   "source": [
    "class ParallelWaveGANGenerator(torch.nn.Module):\n",
    "    \"\"\"Parallel WaveGAN Generator module.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 out_channels=1,\n",
    "                 kernel_size=3,\n",
    "                 layers=30,\n",
    "                 stacks=3,\n",
    "                 residual_channels=64,\n",
    "                 gate_channels=128,\n",
    "                 skip_channels=64,\n",
    "                 aux_channels=80,\n",
    "                 aux_context_window=2,\n",
    "                 dropout=0.0,\n",
    "                 bias=True,\n",
    "                 use_weight_norm=True,\n",
    "                 use_causal_conv=False,\n",
    "                 upsample_conditional_features=True,\n",
    "                 upsample_net=\"ConvInUpsampleNetwork\",\n",
    "                 upsample_params={\"upsample_scales\": [4, 4, 4, 4]},\n",
    "                 ):\n",
    "        \"\"\"Initialize Parallel WaveGAN Generator module.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int): Kernel size of dilated convolution.\n",
    "            layers (int): Number of residual block layers.\n",
    "            stacks (int): Number of stacks i.e., dilation cycles.\n",
    "            residual_channels (int): Number of channels in residual conv.\n",
    "            gate_channels (int):  Number of channels in gated conv.\n",
    "            skip_channels (int): Number of channels in skip conv.\n",
    "            aux_channels (int): Number of channels for auxiliary feature conv.\n",
    "            aux_context_window (int): Context window size for auxiliary feature.\n",
    "            dropout (float): Dropout rate. 0.0 means no dropout applied.\n",
    "            bias (bool): Whether to use bias parameter in conv layer.\n",
    "            use_weight_norm (bool): Whether to use weight norm.\n",
    "                If set to true, it will be applied to all of the conv layers.\n",
    "            use_causal_conv (bool): Whether to use causal structure.\n",
    "            upsample_conditional_features (bool): Whether to use upsampling network.\n",
    "            upsample_net (str): Upsampling network architecture.\n",
    "            upsample_params (dict): Upsampling network parameters.\n",
    "        \"\"\"\n",
    "        super(ParallelWaveGANGenerator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.aux_channels = aux_channels\n",
    "        self.aux_context_window = aux_context_window\n",
    "        self.layers = layers\n",
    "        self.stacks = stacks\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # check the number of layers and stacks\n",
    "        assert layers % stacks == 0\n",
    "        layers_per_stack = layers // stacks\n",
    "\n",
    "        # define first convolution\n",
    "        self.first_conv = Conv1d1x1(in_channels, residual_channels, bias=True)\n",
    "\n",
    "        # define conv + upsampling network\n",
    "        if upsample_conditional_features:\n",
    "            upsample_params.update({\n",
    "                \"use_causal_conv\": use_causal_conv,\n",
    "            })\n",
    "            if upsample_net == \"MelGANGenerator\":\n",
    "                assert aux_context_window == 0\n",
    "                upsample_params.update({\n",
    "                    \"use_weight_norm\": False,  # not to apply twice\n",
    "                    \"use_final_nonlinear_activation\": False,\n",
    "                })\n",
    "                self.upsample_net = getattr(models, upsample_net)(**upsample_params)\n",
    "            else:\n",
    "                if upsample_net == \"ConvInUpsampleNetwork\":\n",
    "                    upsample_params.update({\n",
    "                        \"aux_channels\": aux_channels,\n",
    "                        \"aux_context_window\": aux_context_window,\n",
    "                    })\n",
    "                self.upsample_net = ConvInUpsampleNetwork(**upsample_params)\n",
    "            self.upsample_factor = np.prod(upsample_params[\"upsample_scales\"])\n",
    "        else:\n",
    "            self.upsample_net = None\n",
    "            self.upsample_factor = 1\n",
    "\n",
    "        # define residual blocks\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            dilation = 2 ** (layer % layers_per_stack)\n",
    "            conv = ResidualBlock(\n",
    "                kernel_size=kernel_size,\n",
    "                residual_channels=residual_channels,\n",
    "                gate_channels=gate_channels,\n",
    "                skip_channels=skip_channels,\n",
    "                aux_channels=aux_channels,\n",
    "                dilation=dilation,\n",
    "                dropout=dropout,\n",
    "                bias=bias,\n",
    "                use_causal_conv=use_causal_conv,\n",
    "            )\n",
    "            self.conv_layers += [conv]\n",
    "\n",
    "        # define output layers\n",
    "        self.last_conv_layers = torch.nn.ModuleList([\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            Conv1d1x1(skip_channels, skip_channels, bias=True),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            Conv1d1x1(skip_channels, out_channels, bias=True),\n",
    "        ])\n",
    "\n",
    "        # apply weight norm\n",
    "        if use_weight_norm:\n",
    "            self.apply_weight_norm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "        Args:\n",
    "            x (Tensor): Input noise signal (B, 1, T).\n",
    "            c (Tensor): Local conditioning auxiliary features (B, C ,T').\n",
    "        Returns:\n",
    "            Tensor: Output tensor (B, out_channels, T)\n",
    "        \"\"\"\n",
    "        # encode to hidden representation\n",
    "        c = None\n",
    "        x = self.first_conv(x)\n",
    "        skips = 0\n",
    "        for f in self.conv_layers:\n",
    "            x, h = f(x, c)\n",
    "            skips += h\n",
    "        skips *= math.sqrt(1.0 / len(self.conv_layers))\n",
    "\n",
    "        # apply final layers\n",
    "        x = skips\n",
    "        for f in self.last_conv_layers:\n",
    "            x = f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        \"\"\"Remove weight normalization module from all of the layers.\"\"\"\n",
    "        def _remove_weight_norm(m):\n",
    "            try:\n",
    "                torch.nn.utils.remove_weight_norm(m)\n",
    "            except ValueError:  # this module didn't have weight norm\n",
    "                return\n",
    "\n",
    "        self.apply(_remove_weight_norm)\n",
    "\n",
    "    def apply_weight_norm(self):\n",
    "            \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n",
    "            def _apply_weight_norm(m):\n",
    "                if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.Conv2d):\n",
    "                    torch.nn.utils.weight_norm(m)\n",
    "            self.apply(_apply_weight_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmHwqdNQ8FLQ"
   },
   "source": [
    "## Intialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUeMB8V479fH"
   },
   "outputs": [],
   "source": [
    "model = ParallelWaveGANGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWyTiJkE8LkH"
   },
   "source": [
    "## Download and Load Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awZ1nTVW8OnR"
   },
   "source": [
    "Pre-trained weights on LJSpeech dataset are provided by this [Repository](https://github.com/kan-bayashi/ParallelWaveGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQMBPftD8KEP"
   },
   "outputs": [],
   "source": [
    "# Download Weights\n",
    "!gdown --id 1FBc28i4akuzc8Zbb9f0jIEyyjtJqvrM2 -O checkpoint-400000steps.pkl\n",
    "\n",
    "# Load Weights\n",
    "torch_checkpoints = torch.load(\"/content/checkpoint-400000steps.pkl\", map_location=torch.device('cpu'))\n",
    "torch_model.load_state_dict(torch_checkpoints[\"model\"][\"generator\"])\n",
    "torch_model.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAiU-rQa9zaZ"
   },
   "source": [
    "## Inference with PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uZ70ArMZ96-g"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 100)\n",
    "\n",
    "torch_out = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbqXY2HK_iOo"
   },
   "source": [
    "## Conversion to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e5CysvC94Pg"
   },
   "source": [
    "### Export to ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Xz7g3YN5924V"
   },
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"parallel_wavegan.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                   dynamic_axes={'input' : {1 : 'time'},    # variable lenght axes\n",
    "                                'output' : {1 : 'time'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzS8Z6Pn-j3i"
   },
   "source": [
    "### ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_ic_3Vp-Mjh",
    "outputId": "ebe644c6-f316-4c85-ea76-9d08580e66eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checked succesfully\n"
     ]
    }
   ],
   "source": [
    "store_out = torch_out.detach().numpy()\n",
    "onnx_model = onnx.load(\"parallel_wavegan.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "onnx_runtime_input = x.detach().numpy()\n",
    "\n",
    "print(\"Model checked succesfully\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"parallel_wavegan.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    print(tensor)\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "# # compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: onnx_runtime_input}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh3iV25v_IRe"
   },
   "source": [
    "### Compare PyTorch and ONNX output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4QA0oPnk-yZb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    np.testing.assert_allclose(store_out, ort_outs[0], rtol=1e-05, atol=1e-5)\n",
    "except AssertionError:\n",
    "    print(\"Outputs are not matched. Please check the conversion process again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBkddo7g_pRc"
   },
   "source": [
    "### Convert ONNX model to TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVmET_2n_Fvy",
    "outputId": "cdbd66ba-aac6-41c9-c770-13ca9eb4bc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: parallel_wavegan.pb/assets\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load('parallel_wavegan.onnx')\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_rep.export_graph('parallel_wavegan.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYZ14BmU__Wm"
   },
   "source": [
    "### Convert TF Model to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_J9MF_cDAoZx"
   },
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load('parallel_wavegan.pb')\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('parallel_wavegan.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bFs1EHPM_4Lz"
   },
   "outputs": [],
   "source": [
    "def convert_to_tflite(quantization):\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if quantization == 'float16':\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    tf_lite_model = converter.convert()\n",
    "    model_name = f'parallel_wavegan_{quantization}.tflite'\n",
    "    with open(model_name, 'wb') as f:\n",
    "      f.write(tf_lite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WucdzWSrBP87"
   },
   "source": [
    "#### Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "K6_jUrM_Agqu"
   },
   "outputs": [],
   "source": [
    "quantization = 'dr' #@param [\"dr\", \"float16\"]\n",
    "convert_to_tflite(quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNM9GoOGBUdq",
    "outputId": "979f0354-88cc-4409-a1ca-e15708692026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2M\tparallel_wavegan_dr.tflite\n"
     ]
    }
   ],
   "source": [
    "!du -sh parallel_wavegan_dr.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqKlMLUoBpPR"
   },
   "source": [
    "#### Float16 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMbL00hNBna-",
    "outputId": "312b3a8b-e5b9-4195-8f73-41fbfbacda00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\tparallel_wavegan_float16.tflite\n"
     ]
    }
   ],
   "source": [
    "quantization = 'float16'\n",
    "convert_to_tflite(quantization)\n",
    "!du -sh parallel_wavegan_float16.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sL-oU9J-HF_u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Parallel_WaveGAN_TFLite.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
